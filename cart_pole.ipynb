{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "public-wisconsin",
   "metadata": {},
   "source": [
    "# Solving Cart Pole\n",
    "\n",
    "The goal of this jupyter notebook is to explain basic concepts about reinforcement learning (rl) by solving the [Cart Pole problem](https://gym.openai.com/envs/CartPole-v1/) (The hello world of reinforcement learning).\n",
    "\n",
    "**Disclaimer:** To fully profit from this tutorial it is useful to have some basic understanding about reinforcement learning (You should know about the concepts and use of rewards, observations and environments). If you are completely new to rl, check out this [video](https://www.youtube.com/watch?v=2pWv7GOvuf0) or this [article](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html))\n",
    "\n",
    "Lets start with some imports and a helper function to animate the behaviours of our reinforcement learning agent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "mpl.rc('animation', html='jshtml')\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "# Define some helper functions :-)\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-nurse",
   "metadata": {},
   "source": [
    "## OpenAI gym environments\n",
    "\n",
    "To train and use a rl algorithm, we first need to have an environment. \n",
    "For example: If you want to teach a reinforcement learning agent to play an Atari game -  you first need to have an environment which simulates this Atari game, so the agent can interact with that virtual Atari environment. Or another example: If you want to teach a reinforcement learning agent to play table soccer - you first need to have an environment which can be used by the agent to learn playing.\n",
    "\n",
    "Luckily, OpenAI Gym provides loads of simulated environments to test rl algorithms. One of them is the [cart pole environment](https://gym.openai.com/envs/CartPole-v0/). \n",
    "\n",
    "**It's highly recommended to read up on OpenAI gym environments [here](https://gym.openai.com/docs/) to get started !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-compact",
   "metadata": {},
   "source": [
    "## The Cart Pole environment\n",
    "\n",
    "The [cart pole environment](https://gym.openai.com/envs/CartPole-v0/) contains a cart which can be controlled by moving it either left or right. On top of that cart, there is a pole which we want to balance. The goal is to train an reinforcemnt learning agent which learns to balance the pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-approach",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the openAI Gym environment\n",
    "env = gym.make('CartPole-v1')\n",
    "# Initiallize the environment by calling reset\n",
    "env.reset()\n",
    "# plot the environment by calling the render method\n",
    "img = env.render(mode=\"rgb_array\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-treaty",
   "metadata": {},
   "source": [
    "To perform an action in an OpenAI environment one just has to call the `.step()` method (Parameter defines the action to take). In the Card Pole environment we have 2 actions (A discrete 2-dimensional action space). Balancing the card to the left (Action=0) or balancing to the right (Action=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets play one step - meaning: We are balancing the cart a bit to the left (Action=0 means left).\n",
    "observation, reward, done, info = env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-psychology",
   "metadata": {},
   "source": [
    "As an response to an action performed, the `step()` function returns some information and state of the environment containing:\n",
    "1. Observation: Containing new state information about the environment. One observation is composed as follows:\n",
    "\n",
    "Pos | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "2. Reward: A scalar reward which can be used to train a Reinforcement Learning algorithm (remember: The goal of an RL agent is to maximise rewards). In the Card Pole environment we get a 1.0 Reward for every time step that the card pole does not tip over. **The ultimate goal is to maximise the sum of rewards (The more reward, the longer we balanced the pole!!)** A reward of 200 means we balanced the pole over a maximum time span (An episode automatically terminates after 200 time steps)\n",
    "3. Done: Is True if the episode ended (In this case, after the pole tipped too far to be balanced - or after 200 timesteps are over.)\n",
    "4. info: Further information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-stereo",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-holder",
   "metadata": {},
   "source": [
    "**Pro Tip:** Every OpenAI gym environment implements `env.observation_space` and `env.action_space` which you can read out to get information of how to interact with the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-wednesday",
   "metadata": {},
   "source": [
    "## Test the environment\n",
    "\n",
    "\n",
    "To sanity check the environment, we use a loop to balance to one site multiple times. We expect the pole to tip to the oposite site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset again to denote a new start of a new episode\n",
    "obs = env.reset()\n",
    "\n",
    "#\n",
    "frames = []\n",
    "\n",
    "# Balance to the left for 15 timesteps\n",
    "for i in range(15):\n",
    "    # type of action we perform is 0, meaning we going left\n",
    "    env.step(0)\n",
    "    frames.append(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-hello",
   "metadata": {},
   "source": [
    "... And yes. The pole tips. Now we understood the environemnt and we can try to balance the pole :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-prince",
   "metadata": {},
   "source": [
    "## Hardcoded Policy\n",
    "\n",
    "lets first try to solve Cart Pole by implementing a hard coded policy - without the use of any rl. We just look at the angle of the pole and try to balance it by choosing an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    if angle < 0:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "obs = env.reset()\n",
    "i = 0 \n",
    "for step in range(200):\n",
    "    i += 1\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    action = basic_policy(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-porter",
   "metadata": {},
   "source": [
    "... As you can see, the episode mostly ends at around 40 steps (Pole tips over to far to be balanced - so the environment returns `done=True`).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-bundle",
   "metadata": {},
   "source": [
    "# Q-learning from scratch\n",
    "\n",
    "Before we jump into the implementation, lets clarify some basics:\n",
    "\n",
    "**What is actually a Q-Value?**\n",
    "\n",
    "A Q-Value (Quality value) $Q(s,a)$ denotes the sum of discounted future rewards an agent can expect after it chooses to take action $a$ in state $s$.\n",
    "\n",
    "**What do we mean by discounted future rewards?**\n",
    "\n",
    "We are using a discount factor $\\gamma$ to discount rewards which we are expecting to get in the future.\n",
    "\n",
    "If this explanation is not enough (probably its not enough ðŸ¤¯), please make sure to check out David Silvers introduction to RL video: https://www.youtube.com/watch?v=2pWv7GOvuf0\n",
    "\n",
    "\n",
    "---------\n",
    "\n",
    "**Disclaimer:** Almost all the content from this section is derived or copied from https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb which is an excellent source !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# clear\n",
    "keras.backend.clear_session()\n",
    "initial_observation = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-convertible",
   "metadata": {},
   "source": [
    "The following neural net is used as Deep-Q-Network (DQN). Given a state of the environment, it will estimate a Q-value for each possible action for that particular state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [4] # == env.observation_space.shape\n",
    "n_outputs = 2 # == env.action_space.n\n",
    "\n",
    "dqn = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-black",
   "metadata": {},
   "source": [
    "To select an action using this DQN, we just pick the action with the largest predicted Q-value. However, to ensure that the agent explores the environment, we choose a random action with probability epsilon. This is called epsilon greedy strategy... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        Q_values = dqn.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-astronomy",
   "metadata": {},
   "source": [
    "We will also need a replay memory. It will contain the agent's experiences, in the form of tuples: (obs, action, reward, next_obs, done). Experiences in the replay_memory are later used to train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=2000)\n",
    "\n",
    "# Convenience function to draw random batch from replay_memory\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "    batch = [replay_memory[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-columbia",
   "metadata": {},
   "source": [
    "Now we can create a function that will use the DQN to play one step, and record its experience in the replay memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_memory.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-schema",
   "metadata": {},
   "source": [
    "Below we define the `training_step()` function which is used to train the DQN.\n",
    "\n",
    "According to the Bellmann Optimality Equation one can define a Q-value for a state action pair $Q(s,a)$ as the following:\n",
    "\n",
    "$Q(s,a) = R+\\gamma * \\underset{a'}{\\max} Q(s', a')$\n",
    "\n",
    "This means: A Q-value for a state action pair $Q(s,a)$ equals the immediate reward for taking this action in this particular state $(R)$ plus the sum of discounted future rewards it expects to get ($\\gamma * \\underset{a'}{\\max} Q(s', a')$). \n",
    "\n",
    "This means: \n",
    "If we take an experience from our batch, we can calculate the Q-values of the current state by:\n",
    "\n",
    "**1.)** Either using the DQN to estimate the Q-values for all possible actions given the current state\n",
    "\n",
    "**2.)** Or Using the DQN to estimate the Q-values for the `next_state`, discount it with $\\gamma$ and sum it with the reward (Bellmann Optimality Equation)\n",
    "\n",
    "Both estimates should be the same according to the Bellmann Optimality Equation. The goal during the training of the DQN is to reduce the mean squared error between **1.** and **2.**. Doing so, the DQN gets better in estimating Q-values and thus better in solving the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=1e-2)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    # draw a random batch of experiences\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "    \n",
    "    # use DQN to predict Q values for all possible actions from the next state.\n",
    "    next_Q_values = dqn.predict(next_states)\n",
    "    \n",
    "    # only use the maximum Q value as this will be the action which will be selected.\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    \n",
    "    # calculate target_Q_value based on the Bellmann Optimality equation\n",
    "    target_Q_values = (rewards + (1 - dones) * discount_rate * max_next_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Directly estimate Q-values for actions of the current state\n",
    "        Q_values = dqn(states)\n",
    "        Q_values = tf.reduce_sum(Q_values * mask, axis=1, keepdims=True)\n",
    "        # calculate loss based on mse \n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    # perform gradient descent step to train the neural net\n",
    "    grads = tape.gradient(loss, dqn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, dqn.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-mounting",
   "metadata": {},
   "source": [
    "Finally we are ready to train our DQN. Details can be found in the comments in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seeds helps to reproduce results\n",
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "rewards = [] \n",
    "best_score = 0\n",
    "\n",
    "# we are training our model for 600 episodes !\n",
    "for episode in range(600):\n",
    "    # at the beginning of each episode we reset the environment to start fresh again :-)\n",
    "    obs = env.reset()    \n",
    "    # As the env resets automatically at step 200, its enough to iterate for max 200 steps.\n",
    "    for step in range(200):\n",
    "        # epsilon denotes the probability to explore rather than to take the action with the highest Q-value. \n",
    "        # If this is not clear, make sure to read about epsilon greedy strategies\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        # Calling the play_one_step method to fill our replay_memory with experiences\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        # In case the pole tips to far, we lost and the episode ends :-(\n",
    "        if done:\n",
    "            break\n",
    "    # We keep track of number of steps played of each episode to analyse the training process later\n",
    "    rewards.append(step)\n",
    "    \n",
    "    # We run one training step after episode 50. (So the replay_buffer is full)\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "        \n",
    "    # Making sure to keep track of the model with the best weights. \n",
    "    if step > best_score:\n",
    "        best_weights = dqn.get_weights()\n",
    "        best_score = step\n",
    "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\") # Not shown\n",
    "\n",
    "dqn.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-office",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.title('Rewards per training episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-excellence",
   "metadata": {},
   "source": [
    "Last but not least lets have a look at our trained agent in action :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "frames = []\n",
    "\n",
    "for step in range(200):\n",
    "    Q_values = dqn.predict(state[np.newaxis])\n",
    "    action = np.argmax(Q_values[0])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    \n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-owner",
   "metadata": {},
   "source": [
    "# ChainerRL \n",
    "\n",
    "Lets see how the Task of balancing the pole could be solved with the RL framework chainerRL\n",
    "\n",
    "------\n",
    "\n",
    "**Disclaimer** Almost all of the content of this section is derived or copied from: https://github.com/chainer/chainerrl/blob/master/examples/quickstart/quickstart.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainerrl\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "_q_func = chainerrl.q_functions.FCStateQFunctionWithDiscreteAction(\n",
    "    obs_size, n_actions,\n",
    "    n_hidden_layers=2, n_hidden_channels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Adam to optimize q_func. eps=1e-2 is for stability.\n",
    "optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "optimizer.setup(_q_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the discount factor that discounts future rewards.\n",
    "gamma = 0.95\n",
    "\n",
    "# Use epsilon-greedy for exploration\n",
    "explorer = chainerrl.explorers.ConstantEpsilonGreedy(\n",
    "    epsilon=0.3, random_action_func=env.action_space.sample)\n",
    "\n",
    "# DQN uses Experience Replay.\n",
    "# Specify a replay buffer and its capacity.\n",
    "replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=2000)\n",
    "\n",
    "# Since observations from CartPole-v0 is numpy.float64 while\n",
    "# Chainer only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi.\n",
    "phi = lambda x: x.astype(np.float32, copy=False)\n",
    "\n",
    "# Now create an agent that will interact with the environment.\n",
    "agent = chainerrl.agents.DoubleDQN(\n",
    "    _q_func, optimizer, replay_buffer, gamma, explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "for episode in range(600):\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    for step in range(200):\n",
    "        # Uncomment to watch the behaviour\n",
    "        # env.render()\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    # We keep track of number of steps played of each episode to analyse the training process later\n",
    "    rewards.append(step)\n",
    "    \n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    print(\"\\rEpisode: {}, Steps: {}\".format(episode, step + 1), end=\"\") # Not shown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "frames = []\n",
    "\n",
    "for step in range(200):\n",
    "    action = agent.act(obs)\n",
    "    obs, r, done, _ = env.step(action)\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "agent.stop_episode()\n",
    "\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-scottish",
   "metadata": {},
   "source": [
    "Training with chainerRL can be further simplified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-supervisor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chainerrl.experiments.train_agent_with_evaluation(\n",
    "    agent, env,\n",
    "    steps=10000,\n",
    "    eval_n_steps = None,# Train the agent for 2000 steps\n",
    "    eval_n_episodes=10,       # 10 episodes are sampled for each evaluation\n",
    "    train_max_episode_len=2000,  # Maximum length of each episode\n",
    "    eval_interval=3300,   # Evaluate the agent after every 1000 steps\n",
    "    outdir='result')      # Save everything to 'result' directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-composite",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb\n",
    "- https://www.youtube.com/watch?v=2pWv7GOvuf0\n",
    "- https://github.com/chainer/chainerrl/blob/master/examples/quickstart/quickstart.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
